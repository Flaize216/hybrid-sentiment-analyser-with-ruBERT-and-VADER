import torch
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    Trainer, TrainingArguments, DataCollatorWithPadding
)
from datasets import load_dataset, Dataset, DatasetDict
import numpy as np
from sklearn.metrics import accuracy_score, f1_score
import os

def main():

    # üîß –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –î–õ–Ø –ú–ê–õ–ï–ù–¨–ö–û–ô –ú–û–î–ï–õ–ò
    class FastModelConfig:
        # üéØ –ú–ê–õ–ï–ù–¨–ö–ò–ï –ò –ë–´–°–¢–†–´–ï –ú–û–î–ï–õ–ò (–≤—ã–±–µ—Ä–∏ –æ–¥–Ω—É)
        model_options = {
            "rubert-tiny": "cointegrated/rubert-tiny2",  # ‚ö° –û–ß–ï–ù–¨ –ë–´–°–¢–†–ê–Ø
            "distilrubert": "ai-forever/ruDistilBert",   # ‚ö° –ë–´–°–¢–†–ê–Ø
            "rubert-small": "sberbank-ai/ruBert-base",   # üöÄ –ë–ê–õ–ê–ù–°
            "multilingual": "bert-base-multilingual-uncased"  # üåç –ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω–∞—è
        }
        
        # –í—ã–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª—å (–ø–æ–º–µ–Ω—è–π –Ω–∞ –Ω—É–∂–Ω—É—é)
        model_name = model_options["rubert-small"]
        dataset_name = "MonoHime/ru_sentiment_dataset"
        
        # –ù–ê–°–¢–†–û–ô–ö–ò –î–õ–Ø –°–ö–û–†–û–°–¢–ò
        batch_size = 16             # –ë–æ–ª—å—à–æ–π –±–∞—Ç—á –¥–ª—è –º–∞–ª–µ–Ω—å–∫–æ–π –º–æ–¥–µ–ª–∏
        max_length = 256            # –ú–æ–∂–Ω–æ –±–æ–ª—å—à–µ —Ç–µ–∫—Å—Ç–∞
        gradient_accumulation =4   # –ú–∞–ª–µ–Ω—å–∫–∏–π accumulation
        learning_rate = 3e-5        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π LR
        num_epochs = 2              # –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ
        fp16 = True
        
        # üíæ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        warmup_ratio = 0.05
        weight_decay = 0.01

    config = FastModelConfig()

    print(f"üéØ –í—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å: {config.model_name}")

    print("=== –ó–ê–ì–†–£–ó–ö–ê –î–ê–¢–ê–°–ï–¢–ê ===")

    def load_dataset_fast():
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            dataset = load_dataset(config.dataset_name)
            print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {config.dataset_name}")
            
            # üîß –ú–û–ñ–ï–ú –ü–û–ó–í–û–õ–ò–¢–¨ –°–ï–ë–ï –ë–û–õ–¨–®–ï –î–ê–ù–ù–´–•
            max_train_samples = 20000  # 100–∫ –ø—Ä–∏–º–µ—Ä–æ–≤!
            max_eval_samples = 2000    # 10–∫ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            
            if len(dataset['train']) > max_train_samples:
                print(f"üîÑ –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º train —Å {len(dataset['train'])} –¥–æ {max_train_samples} –ø—Ä–∏–º–µ—Ä–æ–≤")
                dataset['train'] = dataset['train'].select(range(max_train_samples))
            
            # –°–æ–∑–¥–∞–µ–º validation split –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if 'validation' not in dataset and 'valid' not in dataset and 'val' not in dataset:
                print("üîÑ –°–æ–∑–¥–∞–µ–º validation split...")
                train_valid_split = dataset['train'].train_test_split(
                    test_size=0.1,
                    seed=42
                )
                dataset = DatasetDict({
                    'train': train_valid_split['train'],
                    'validation': train_valid_split['test']
                })
                eval_split = 'validation'
            else:
                eval_split = 'validation' if 'validation' in dataset else 'valid'
                if len(dataset[eval_split]) > max_eval_samples:
                    print(f"üîÑ –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º {eval_split} —Å {len(dataset[eval_split])} –¥–æ {max_eval_samples} –ø—Ä–∏–º–µ—Ä–æ–≤")
                    dataset[eval_split] = dataset[eval_split].select(range(max_eval_samples))
            
            print(f"\n –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã:")
            print(f"   Train: {len(dataset['train'])} –ø—Ä–∏–º–µ—Ä–æ–≤")
            print(f"   Eval ({eval_split}): {len(dataset[eval_split])} –ø—Ä–∏–º–µ—Ä–æ–≤")
            
            return dataset, eval_split
            
        except Exception as e:
            print(f" –û—à–∏–±–∫–∞: {e}")
            # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
            train_data = {
                "text": ["–û—Ç–ª–∏—á–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç!", "–£–∂–∞—Å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ", "–ù–æ—Ä–º–∞–ª—å–Ω–æ"] * 5000,
                "sentiment": [1, 2, 0] * 5000  
            }
            valid_data = {
                "text": ["–ü—Ä–µ–∫—Ä–∞—Å–Ω–æ", "–ü–ª–æ—Ö–æ", "–°—Ä–µ–¥–Ω–µ"] * 500,
                "sentiment": [1, 2, 0] * 500  
            }
            
            dataset = DatasetDict({
                'train': Dataset.from_dict(train_data),
                'validation': Dataset.from_dict(valid_data)
            })
            print(f"‚úÖ –°–æ–∑–¥–∞–Ω —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {len(dataset['train'])} train, {len(dataset['validation'])} validation")
            return dataset, 'validation'

    dataset, eval_split = load_dataset_fast()

    print("\n=== –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ===")

    # –§–£–ù–ö–¶–ò–Ø –ü–†–ï–û–ë–†–ê–ó–û–í–ê–ù–ò–Ø –ú–ï–¢–û–ö
    def convert_labels_fast(example):
        sentiment = example.get('sentiment')
        if isinstance(sentiment, (int, float)):
            sentiment_int = int(sentiment)
            # üéØ –ü–†–ï–û–ë–†–ê–ó–û–í–ê–ù–ò–ï –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞ (0=NEUTRAL, 1=POSITIVE, 2=NEGATIVE)
            if sentiment_int == 0:
                example['labels'] = 1  # NEUTRAL ‚Üí 1
            elif sentiment_int == 1:
                example['labels'] = 2  # POSITIVE ‚Üí 2
            elif sentiment_int == 2:
                example['labels'] = 0  # NEGATIVE ‚Üí 0
            else:
                example['labels'] = 1  # NEUTRAL –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        elif isinstance(sentiment, str):
            sentiment_lower = sentiment.lower()
            if any(word in sentiment_lower for word in ['negative', '–Ω–µ–≥–∞—Ç–∏–≤', 'neg', '–ø–ª–æ—Ö', '—É–∂–∞—Å']):
                example['labels'] = 0  # NEGATIVE
            elif any(word in sentiment_lower for word in ['positive', '–ø–æ–∑–∏—Ç–∏–≤', 'pos', '—Ö–æ—Ä–æ—à', '–æ—Ç–ª–∏—á–Ω', '–ø—Ä–µ–∫—Ä–∞—Å–Ω']):
                example['labels'] = 2  # POSITIVE
            else:
                example['labels'] = 1  # NEUTRAL
        else:
            example['labels'] = 1  # NEUTRAL –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        return example

    print("üîÑ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫...")
    dataset = dataset.map(convert_labels_fast)

    print(f"\n=== –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò ===")

    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token or tokenizer.unk_token

    # üîß –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò
    model = AutoModelForSequenceClassification.from_pretrained(
        config.model_name,
        num_labels=3,
        id2label={0: "NEGATIVE", 1: "NEUTRAL", 2: "POSITIVE"},
        label2id={"NEGATIVE": 0, "NEUTRAL": 1, "POSITIVE": 2}
    )
    print(f"‚úÖ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {config.model_name}")

    # üîß–¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø
    def tokenize_fast(examples):
        texts = [str(text) for text in examples["text"]]
        return tokenizer(
            texts,
            padding=True,
            truncation=True,
            max_length=config.max_length,
            return_tensors=None
        )

    print("‚ö° —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è...")
    tokenized_datasets = dataset.map(
        tokenize_fast, 
        batched=True,
        batch_size=2000,  # üöÄ –û—á–µ–Ω—å –±–æ–ª—å—à–∏–µ –±–∞—Ç—á–∏ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        remove_columns=['text', 'sentiment']
    )

    print(f"üìä –î–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã: {len(tokenized_datasets['train'])} train, {len(tokenized_datasets[eval_split])} eval")

    # üîß DATA COLLATOR
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    # üîß –ú–ï–¢–†–ò–ö–ò
    def compute_metrics_fast(eval_pred):
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=1)
        return {
            "accuracy": accuracy_score(labels, predictions),
            "f1_macro": f1_score(labels, predictions, average="macro")
        }

    # üîß –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ï –ê–†–ì–£–ú–ï–ù–¢–´ –û–ë–£–ß–ï–ù–ò–Ø (–ë–ï–ó MULTIPROCESSING)
    training_args = TrainingArguments(
        output_dir="./fast-model-sentiment",
        overwrite_output_dir=True,
        
        # ‚ö° –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ê–Ø –°–ö–û–†–û–°–¢–¨
        per_device_train_batch_size=config.batch_size,
        per_device_eval_batch_size=config.batch_size * 2,
        gradient_accumulation_steps=config.gradient_accumulation,
        fp16=config.fp16,
        
        # üéØ –ë–´–°–¢–†–û–ï –û–ë–£–ß–ï–ù–ò–ï
        num_train_epochs=config.num_epochs,
        learning_rate=config.learning_rate,
        warmup_ratio=config.warmup_ratio,
        weight_decay=config.weight_decay,
        
        # üìä –í–ê–õ–ò–î–ê–¶–ò–Ø
        eval_strategy="epoch", 
        save_strategy="epoch",
        logging_steps=50,
        load_best_model_at_end=True,
        metric_for_best_model="f1_macro",
        
        # ‚ö° –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò (–í–´–ö–õ–Æ–ß–ê–ï–ú MULTIPROCESSING –î–õ–Ø WINDOWS)
        dataloader_pin_memory=False,  # üîß –í–´–ö–õ–Æ–ß–ê–ï–ú –¥–ª—è Windows
        dataloader_num_workers=0,     # üîß –í–´–ö–õ–Æ–ß–ê–ï–ú workers –¥–ª—è Windows
        optim="adamw_torch",
        remove_unused_columns=True,
        label_names=["labels"],
        
        # üíæ –°–û–•–†–ê–ù–ï–ù–ò–ï
        save_total_limit=2,
    )

    # üîß TRAINER
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets[eval_split],
        data_collator=data_collator,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics_fast,
    )

    print(f"\nüéØ –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ú–û–î–ï–õ–ò:")
    print(f"   –ú–æ–¥–µ–ª—å: {config.model_name}")
    print(f"   Train examples: {len(tokenized_datasets['train'])}")
    print(f"   Batch size: {config.batch_size}")
    print(f"   Effective batch: {config.batch_size * config.gradient_accumulation}")
    print(f"   Epochs: {config.num_epochs}")
    print(f"   Max length: {config.max_length}")

    # üîß –†–ê–°–ß–ï–¢ –í–†–ï–ú–ï–ù–ò
    total_steps = len(tokenized_datasets['train']) * config.num_epochs / (config.batch_size * config.gradient_accumulation)
    estimated_time = total_steps / 50  # –ü—Ä–∏–º–µ—Ä–Ω–æ 200 —à–∞–≥–æ–≤ –≤ –º–∏–Ω—É—Ç—É –¥–ª—è  –º–æ–¥–µ–ª–∏
    print(f"   Estimated time: ~{estimated_time:.1f} –º–∏–Ω—É—Ç")

    print("\nüöÄ –ó–ê–ü–£–°–ö –ë–´–°–¢–†–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø...")

    try:
        import time
        start_time = time.time()
        
        # –û–±—É—á–∞–µ–º —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
        train_result = trainer.train()
        
        end_time = time.time()
        training_time = (end_time - start_time) / 60
        
        print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {training_time:.1f} –º–∏–Ω—É—Ç!")
        print(f"üìà Final train loss: {train_result.metrics['train_loss']:.4f}")
        
        # üîß –§–ò–ù–ê–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê
        print(f"\nüìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê –ù–ê {eval_split.upper()}:")
        eval_results = trainer.evaluate()
        for key, value in eval_results.items():
            print(f"   {key}: {value:.4f}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        trainer.save_model("./trained-distil-sentiment-model")
        print("üíæ –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
        import traceback
        traceback.print_exc()

    # üîß –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï
    def predict_sentiment_fast(texts, model, tokenizer):
        model.eval()
        device = next(model.parameters()).device
        
        texts = [str(text) for text in texts]
        inputs = tokenizer(
            texts, 
            return_tensors="pt", 
            padding=True, 
            truncation=True, 
            max_length=config.max_length
        )
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model(**inputs)
        
        probabilities = torch.nn.functional.softmax(outputs.logits.cpu(), dim=-1)
        predicted_classes = torch.argmax(probabilities, dim=1)
        confidences = probabilities[torch.arange(len(texts)), predicted_classes]
        
        labels = ["NEGATIVE", "NEUTRAL", "POSITIVE"]
        
        return [
            {"text": text, "sentiment": labels[pred.item()], "confidence": conf.item()}
            for text, pred, conf in zip(texts, predicted_classes, confidences)
        ]

    print("\n=== –¢–ï–°–¢ –ú–û–î–ï–õ–ò ===")
    test_texts = [
        "–≠—Ç–æ –ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç! –û—á–µ–Ω—å –¥–æ–≤–æ–ª–µ–Ω –ø–æ–∫—É–ø–∫–æ–π.",
        "–£–∂–∞—Å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –Ω–∏–∫–æ–≥–¥–∞ –±–æ–ª—å—à–µ –Ω–µ –∫—É–ø–ª—é.",
        "–ù–æ—Ä–º–∞–ª—å–Ω–æ, –Ω–∏—á–µ–≥–æ –æ—Å–æ–±–µ–Ω–Ω–æ–≥–æ. –ú–æ–∂–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è.",
        "–í–æ—Å—Ö–∏—Ç–∏—Ç–µ–ª—å–Ω–æ! –õ—É—á—à–µ–µ —á—Ç–æ —è –≤–∏–¥–µ–ª!",
        "–ü–æ–ª–Ω—ã–π —Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∏–µ, –∑—Ä—è –ø–æ—Ç—Ä–∞—Ç–∏–ª –¥–µ–Ω—å–≥–∏."
    ]

    print("–¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ 5 –ø—Ä–∏–º–µ—Ä–∞—Ö...")
    results = predict_sentiment_fast(test_texts, model, tokenizer)

    print("\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ê:")
    for i, result in enumerate(results, 1):
        emoji = "üòä" if result["sentiment"] == "POSITIVE" else "üòê" if result["sentiment"] == "NEUTRAL" else "üòû"
        print(f"{i}. {emoji} {result['sentiment']:8} ({result['confidence']:.3f}) | {result['text'][:50]}...")


if __name__ == '__main__':
    main()